# 1st_Nueral_Network
My first attempt at creating a Nueral Network
The inputs(list "I") are random and are only for testing.
Uses RELU(Rectified Linear) activation function to calculate/modify inputs going into a hidden layer.
Uses SoftMax activation function for the output(final) layer.
Haven't gotten to finishing the Cross Entropy loss function.
Sorry for the lack of comments will add them soon.
